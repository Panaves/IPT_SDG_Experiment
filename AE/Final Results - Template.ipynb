{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj1 = \"tratamento A\"\n",
    "proj2 = \"tratamento B\"\n",
    "proj3 = \"tratamento C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.A - Installing Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.B - Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import ks_2samp,wasserstein_distance\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly import tools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.C - Custom Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Classes and Functions\n",
    "from classes import xplor, LowerCase,MissingValue, Outliers, CharEncoder, ScalingTreatmeant, Autoencoder, compare_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params):\n",
    "   \n",
    "    l = [params[s] for s in params.keys() if s.startswith('unit')]\n",
    "\n",
    "    return Autoencoder(\n",
    "        layers_= l +  [params['center']],\n",
    "        activations=[params['activation']]*len(l) + [params['center_activation']],\n",
    "        final_act=params['final_activation'],\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=\"adam\",\n",
    "        learning_rate = params['learning_rate']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_plots(df_, df_synth, title):\n",
    "\n",
    "    # Inputs para gerar o grid de gráficos\n",
    "    n_cols = X_test_t2.shape[1]\n",
    "\n",
    "    plot_r = int(np.ceil((n_cols/4)))\n",
    "    plot_c = 4\n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "\n",
    "    # for loop para plotar todas as variáveis\n",
    "    W_List = {}\n",
    "    fig, ax = plt.subplots(plot_r , plot_c, figsize = (4*plot_c,3*plot_r))\n",
    "    for column in df_.columns:\n",
    "        W_List[column] = []\n",
    "\n",
    "        if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "\n",
    "            gb = df_.groupby([column]).size()\n",
    "            gb = np.round((gb/gb.sum())*100,1)\n",
    "\n",
    "            gb_s = df_synth.groupby([column]).size()\n",
    "            gb_s = np.round((gb_s/gb_s.sum())*100,1)\n",
    "\n",
    "            compare = pd.concat([gb,gb_s], axis = 1).fillna(0)\n",
    "            compare.columns = ['Real','Synthetic']\n",
    "            compare['DIFF'] = np.round(abs(compare['Real'] - compare['Synthetic']),2)\n",
    "\n",
    "            compare[['Real','Synthetic']].plot(kind = 'bar', ax = ax[r,c], alpha = 0.8)\n",
    "            ax[r,c].set_title(column+\":\"+ str(compare['DIFF'].abs().sum()))\n",
    "            ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df_[column])\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(le.transform(df_[column]).reshape(-1,1))\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(le.transform(df_synth[column].dropna()).reshape(-1,1)))\n",
    "                                               ,np.squeeze(mms.transform(le.transform(df_      [column]         ).reshape(-1,1)))\n",
    "                                              )\n",
    "                         )\n",
    "\n",
    "        else:           \n",
    "            bins = np.histogram_bin_edges(df_[column], 20)\n",
    "            sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5, label = 'Real')\n",
    "            sns.histplot(data=df_synth, x=column, bins = bins, ax = ax[r,c], color = 'red', alpha = 0.4, label = 'Synthetic')\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(df_[[column]])\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(df_synth[[column]]))\n",
    "                                               ,np.squeeze(mms.transform(df_[[column]]))\n",
    "                                              )\n",
    "                         )\n",
    "    plt.suptitle(title, fontsize = 15)\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_plots2(df_, df_synth, title, c_size = 4, r_size = 4):\n",
    "\n",
    "    # Inputs para gerar o grid de gráficos\n",
    "    n_cols = X_test_t2.shape[1]\n",
    "\n",
    "    plot_r = int(np.ceil((n_cols/4)))\n",
    "    plot_c = 4\n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "\n",
    "    # for loop para plotar todas as variáveis\n",
    "    W_List = {}\n",
    "    fig, ax = plt.subplots(plot_r , plot_c, figsize = (c_size*plot_c,r_size*plot_r))\n",
    "    for column in df_.columns:\n",
    "        W_List[column] = []\n",
    "\n",
    "        if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "\n",
    "            gb = df_.groupby([column]).size()\n",
    "            gb = np.round((gb/gb.sum())*100,1)\n",
    "\n",
    "            gb_s = df_synth.groupby([column]).size()\n",
    "            gb_s = np.round((gb_s/gb_s.sum())*100,1)\n",
    "\n",
    "            compare = pd.concat([gb,gb_s], axis = 1).fillna(0)\n",
    "            compare.columns = ['Real','Synthetic']\n",
    "            compare['DIFF'] = np.round(abs(compare['Real'] - compare['Synthetic']),2)\n",
    "\n",
    "            compare[['Real','Synthetic']].plot(kind = 'bar', color=['blue','red'], ax = ax[r,c], alpha = 0.8, legend = False)\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].set_xticks([])\n",
    "            ax[r,c].set_yticks([])\n",
    "            ax[r,c].set_xlabel(\"\")\n",
    "            ax[r,c].set_ylabel(\"\")\n",
    "            c+=1\n",
    "\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df_[column])\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(le.transform(df_[column]).reshape(-1,1))\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(le.transform(df_synth[column].dropna()).reshape(-1,1)))\n",
    "                                               ,np.squeeze(mms.transform(le.transform(df_      [column]         ).reshape(-1,1)))\n",
    "                                              )\n",
    "                         )\n",
    "\n",
    "        else:           \n",
    "            bins = np.histogram_bin_edges(df_[column], 20)\n",
    "            sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5)\n",
    "            sns.histplot(data=df_synth, x=column, bins = bins, ax = ax[r,c], color = 'red', alpha = 0.4)\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].set_xlabel(\"\")            \n",
    "            ax[r,c].set_xticks([])\n",
    "            ax[r,c].set_yticks([])\n",
    "            ax[r,c].set_ylabel(\"\")\n",
    "            c+=1\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(df_[[column]])\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(df_synth[[column]]))\n",
    "                                               ,np.squeeze(mms.transform(df_[[column]]))\n",
    "                                              )\n",
    "                         )\n",
    "    plt.suptitle(title, fontsize = 15)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_models(real, synth):\n",
    "    \n",
    "    ce = CharEncoder()\n",
    "    \n",
    "    RX_m = real.drop([ 'salary'], axis = 1).copy()\n",
    "    RX_m = ce.fit_transform(RX_m.copy())\n",
    "    RY_m = real[['salary']].copy()\n",
    "\n",
    "    X_trainR, X_testR, Y_trainR, Y_testR  = train_test_split(RX_m, RY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_R = RandomForestClassifier()\n",
    "    RFC_R.fit(X_trainR,Y_trainR)\n",
    "\n",
    "    SX_m = synth.drop([ 'salary'], axis = 1).copy()\n",
    "    SX_m = ce.transform(SX_m.copy())\n",
    "    SX_m.index = RX_m.index\n",
    "\n",
    "    SY_m = synth[['salary']].copy()\n",
    "    SY_m.index = RY_m.index\n",
    "\n",
    "    X_trainS = SX_m.loc[X_trainR.index,:]\n",
    "    X_testS  = SX_m.loc[X_testR.index,:]\n",
    "    Y_trainS = SY_m.loc[Y_trainR.index,:]\n",
    "    Y_testS  = SY_m.loc[Y_testR.index,:]\n",
    "\n",
    "    RFC_S = RandomForestClassifier(max_depth = 8)\n",
    "    RFC_S.fit(X_trainS,Y_trainS)\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    \n",
    "    final_predict = pd.DataFrame({'REAL':RFC_S.predict(X_testR), \"SYNT\":RFC_S.predict(X_testS)}, index = X_testR.index)\n",
    "    final_predict['CHECK'] = final_predict[\"REAL\"] == final_predict[\"SYNT\"]\n",
    "    \n",
    "    real_predict = pd.DataFrame({'PRED':RFC_S.predict(X_testR), \"REAL\":Y_testR['salary']}, index = X_testR.index)\n",
    "    real_predict['CHECK'] = real_predict[\"REAL\"] == real_predict[\"PRED\"]\n",
    "    \n",
    "    real_predict2 = pd.DataFrame({'PRED':RFC_S.predict_proba(X_testR)[:, 1], \"REAL\":Y_testR['salary']}, index = X_testR.index)\n",
    "    \n",
    "    return [final_predict['CHECK'].sum()/final_predict.shape[0] , real_predict['CHECK'].sum()/real_predict.shape[0], roc_auc_score(real_predict2.REAL, real_predict2.PRED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_models_train(real, synth):\n",
    "    \n",
    "    ce = CharEncoder()\n",
    "    \n",
    "    RX_m = real.drop([ 'salary'], axis = 1).copy()\n",
    "    RX_m = ce.fit_transform(RX_m.copy())\n",
    "    RY_m = real[['salary']].copy()\n",
    "\n",
    "    X_trainR, X_testR, Y_trainR, Y_testR  = train_test_split(RX_m, RY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_R = RandomForestClassifier()\n",
    "    RFC_R.fit(X_trainR,Y_trainR)\n",
    "\n",
    "    SX_m = synth.drop([ 'salary'], axis = 1).copy()\n",
    "    SX_m = ce.transform(SX_m.copy())\n",
    "    SX_m.index = RX_m.index\n",
    "\n",
    "    SY_m = synth[['salary']].copy()\n",
    "    SY_m.index = RY_m.index\n",
    "\n",
    "    X_trainS = SX_m.loc[X_trainR.index,:]\n",
    "    X_testS  = SX_m.loc[X_testR.index,:]\n",
    "    Y_trainS = SY_m.loc[Y_trainR.index,:]\n",
    "    Y_testS  = SY_m.loc[Y_testR.index,:]\n",
    "\n",
    "    RFC_S = RandomForestClassifier(max_depth = 8)\n",
    "    RFC_S.fit(X_trainS,Y_trainS)\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    \n",
    "    final_predict = pd.DataFrame({'REAL':RFC_S.predict(X_trainR), \"SYNT\":RFC_S.predict(X_trainS)}, index = X_trainR.index)\n",
    "    final_predict['CHECK'] = final_predict[\"REAL\"] == final_predict[\"SYNT\"]\n",
    "    \n",
    "    real_predict = pd.DataFrame({'PRED':RFC_S.predict(X_trainR), \"REAL\":Y_trainR['salary']}, index = X_trainR.index)\n",
    "    real_predict['CHECK'] = real_predict[\"REAL\"] == real_predict[\"PRED\"]\n",
    "    \n",
    "    real_predict2 = pd.DataFrame({'PRED':RFC_S.predict_proba(X_trainR)[:, 1], \"REAL\":Y_trainR['salary']}, index = X_trainR.index)\n",
    "    \n",
    "    return [final_predict['CHECK'].sum()/final_predict.shape[0] , real_predict['CHECK'].sum()/real_predict.shape[0], roc_auc_score(real_predict2.REAL, real_predict2.PRED)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#od.download(\"https://www.kaggle.com/rameshmehta/credit-risk-analysis/version/1\")\n",
    "data = pd.read_csv(\"adult.csv\", header = None)\n",
    "data.columns = ['age','workclass','fnlwgt','education','education_num','marital_status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['fnlwgt']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando classe para fazer limepza dos dados de forma automatizada\n",
    "# Essa classe consegue analisar e identificar variáveis com baixa qualidade em relação a valores nulos,\n",
    "# variáveis categóricas com muitas variáveis, variáveis do tipo data, variáveis com altíssima variância \n",
    "# e variáveis com variância nula\n",
    "xp = xplor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando variáveis nulas.\n",
    "# É realizado um gráfico de pareto para a % de nulos em cada variável.\n",
    "\n",
    "# De acordo com o parâmetro 'level', serão selecionadas (para exclusão) as variaveis que \n",
    "# ultrapassarem o valor desse parâmetro. Ou seja, nesse acso todas as variáveis com mais de 50%\n",
    "# de nulos serão selecionadas para exclusão\n",
    "\n",
    "# Foram encontradas 21 variáveis.\n",
    "\n",
    "xp.check_nulls(level = 0, select = True)\n",
    "print(xp.nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este próximo método visa identificar variáveis categóricas que possuem uma quantidade alta de valores únicos,\n",
    "# Neste exemplo, toda variável categórica com mais de 20 categrias distintas será selecionada para exclusão.\n",
    "\n",
    "# Neste caso foram encontradas 10 variáveis.\n",
    "\n",
    "# O Gráfico de pareto é mostrado para ajudar na identificação visual\n",
    "\n",
    "xp.check_unique_objects(level_unique = 50,select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O próximo método visa identificar variáveis do tipo data/\n",
    "# Para este experimento, essas variáveis serão excluídas.\n",
    "\n",
    "# Neste exemplo, foram encontradas 5 variáveis\n",
    "\n",
    "xp.check_dates(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste último método, o objetivo é encontrar variáveis que possuam uma variância normalizada muito alta ou nula.\n",
    "# O interessante foi verificar que ela se mostrou últi para encontrar \n",
    "# as duas colunas relacionadas ao ID (com altíssima variância) e uma coluna sem variância.\n",
    "\n",
    "xp.check_var(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para finalizar esse processo, é executado o método 'clean_data',\n",
    "# que vai pegar todas as variáveis identificadas nos métodos anteriores e vai excluí-las da base final\n",
    "\n",
    "new_df = xp.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com isso, foram excluídas 34 variáveis.\n",
    "# A base final agora possúi 39 variáveis.\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.groupby(['education_num','education']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import *\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "df = data.copy()\n",
    "plot_number = 10\n",
    "numerics = df.select_dtypes(exclude = ['object','category']).columns\n",
    "objects  = df.select_dtypes(include = ['object','category']).columns\n",
    "    \n",
    "mms = MinMaxScaler()\n",
    "\n",
    "num_desc = pd.DataFrame(mms.fit_transform(df[numerics]), columns = numerics).describe()\n",
    "\n",
    "obj_desc = df[objects].describe()\n",
    "\n",
    "f0 = figure(num = 0, figsize = (12, 8))#, dpi = 300)\n",
    "#f0.suptitle(\"Oscillation decay\", fontsize=14)\n",
    "ax01 = subplot2grid((2, 2), (0, 0))\n",
    "ax02 = subplot2grid((2, 2), (0, 1))\n",
    "ax03 = subplot2grid((2, 2), (1, 0), colspan=2, rowspan=1)\n",
    "\n",
    "num_desc.loc['std',:].sort_values(ascending = False)[:plot_number].plot(kind = 'bar', ax = ax01)\n",
    "ax01.set_ylabel(\"Variância Normalizada\", fontdict = {'size':15})\n",
    "ax01.set_xlabel(\"Variáveis\", fontdict = {'size':15})\n",
    "ax01.set_title('Variáveis com maior variância normalizada', fontdict = {'size':15})\n",
    "ax01.tick_params(axis='x', labelsize=12, labelrotation = 45)\n",
    "\n",
    "num_desc.loc['std',:].sort_values(ascending = True)[:plot_number].plot(kind = 'bar', ax = ax02)\n",
    "ax02.set_ylabel(\"Variância Normalizada\", fontdict = {'size':15})\n",
    "ax02.set_xlabel(\"Variáveis\", fontdict = {'size':15})\n",
    "ax02.set_title('Variáveis com menor variância normalizada', fontdict = {'size':15})\n",
    "ax02.tick_params(axis='x', labelsize=12, labelrotation = 45)\n",
    "\n",
    "obj_desc.loc['unique',:].sort_values(ascending = False)[:20].plot(kind = 'bar', ax = ax03, label = 'Quantidades')\n",
    "ax03.set_ylabel(\"# Categorias\", fontdict = {'size':15})\n",
    "ax03.set_xlabel(\"Variáveis\", fontdict = {'size':15})\n",
    "ax03.set_title('Variáveis com mais categorias', fontdict = {'size':15})\n",
    "ax03.tick_params(axis='x', labelsize=12, labelrotation = 45)\n",
    "f0.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Data Treatmeant Pipeline (Pre-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop(['education_num'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "pipe_full = joblib.load('pipeline_'+proj1+'.sav')\n",
    "pipe_out15 = joblib.load('pipeline_'+proj2+'.sav')\n",
    "pipe_out30 = joblib.load('pipeline_'+proj3+'.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Train Test Split + Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criação da amostra treino e teste.\n",
    "# Nesse caso existe apenas as bases 'X', não sendo necessário a base 'y', pois o input é o próprio target.\n",
    "# O objetivo do processo é replicar exatamente os dados de entrada.\n",
    "\n",
    "X_train, X_test = train_test_split(new_df.copy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Aqui estamos aplicando o .transform() doa pipeline treinada.\n",
    "# Aqui os dados sofrerão as transformações necessárias para ficarem prontos para o treinamento\n",
    "# da rede neural.\n",
    "\n",
    "# Estou mudando um pequeno parâmetro da pipeline para ter a exclusão dos outliers para a amostra treino\n",
    "#my_pipe.set_params(Outliers__train=True)\n",
    "pipe_out15.set_params(Outliers__train=True)\n",
    "pipe_out30.set_params(Outliers__train=True)\n",
    "X_train_full = pipe_full.transform(X_train.copy())\n",
    "X_train_out15 = pipe_out15.transform(X_train.copy())\n",
    "X_train_out30 = pipe_out30.transform(X_train.copy())\n",
    "\n",
    "# Estou mudando um pequeno parâmetro da pipeline para não ter a exclusão dos outliers na amostra teste\n",
    "#my_pipe.set_params(Outliers__train=False)\n",
    "pipe_out15.set_params(Outliers__train=False)\n",
    "pipe_out30.set_params(Outliers__train=False)\n",
    "X_test_full = pipe_full.transform(X_test.copy())\n",
    "X_test_out15 = pipe_out15.transform(X_test.copy())\n",
    "X_test_out30 = pipe_out30.transform(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_full \",X_train_full.shape)\n",
    "print(\"X_train_out15\",X_train_out15.shape)\n",
    "print(\"X_train_out30\",X_train_out30.shape)\n",
    "\n",
    "\n",
    "print(\"X_test_full \",X_test_full.shape)\n",
    "print(\"X_test_out15\",X_test_out15.shape)\n",
    "print(\"X_test_out30\",X_test_out30.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dtypes = {i:str(X_train.dtypes[i]) for i in X_train.dtypes.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 - AutoEncoder (AE)\n",
    "# Carregando e treinando o melhor AE\n",
    "print(\"Training Model FULL - AutoEncoder (AE)\")\n",
    "with open( proj1 + \"_sparse_1hl_params_model.pkl\", \"rb\") as f:\n",
    "    params_full =  pickle.load(f)\n",
    "ae_full = build_model(params_full)\n",
    "ae_full.fit(X_train_full, epochs = 700)\n",
    "synthetic_full = pipe_full.inverse_transform(ae_full.transform(X_test_full.copy())[0])\n",
    "synthetic_full = synthetic_full.astype(all_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 - AutoEncoder (AE)\n",
    "# Carregando e treinando o melhor AE\n",
    "print(\"Training Model FULL out 1.5 - AutoEncoder (AE)\")\n",
    "with open( proj2 + \"_sparse_1hl_params_model.pkl\", \"rb\") as f:\n",
    "    params_out15 =  pickle.load(f)\n",
    "ae_out15 = build_model(params_out15)\n",
    "ae_out15.fit(X_train_out15, epochs = 700)\n",
    "synthetic_out15 = pipe_out15.inverse_transform(ae_out15.transform(X_test_out15.copy())[0])\n",
    "synthetic_out15 = synthetic_out15.astype(all_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_out15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 - AutoEncoder (AE)\n",
    "# Carregando e treinando o melhor AE\n",
    "print(\"Training Model FULL out 3.0 - AutoEncoder (AE)\")\n",
    "with open( proj3 + \"_sparse_1hl_params_model.pkl\", \"rb\") as f:\n",
    "    params_out30 =  pickle.load(f)\n",
    "ae_out30 = build_model(params_out30)\n",
    "ae_out30.fit(X_train_out30, epochs = 700)\n",
    "synthetic_out30 = pipe_out30.inverse_transform(ae_out30.transform(X_test_out30.copy())[0])\n",
    "synthetic_out30 = synthetic_out30.astype(all_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_out30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07.B - Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t2 = pipe_full.inverse_transform(X_test_full.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculando a distância de Wasserstein para todas as colunas para os três modelos\n",
    "W_df_tst = compare_metric(df_real = X_test_t2\n",
    "                      ,dfs_synth = [synthetic_full, synthetic_out15, synthetic_out30]\n",
    "                      , metric = 'wasserstein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_0: AutoEncoder\n",
    "# Data_1: Sparse AutoEncoder\n",
    "# Data_2: Convolutional AutoEncoder\n",
    "W_df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_df_tst.quantile(q = [0,0.5,1]).append( pd.DataFrame(W_df_tst.mean(), columns = ['avg']).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_plots3(df_, df_synth, title):\n",
    "\n",
    "    # Inputs para gerar o grid de gráficos\n",
    "    n_cols = X_test_t2.shape[1]\n",
    "\n",
    "    plot_c = 2\n",
    "    plot_r = int(np.ceil((n_cols/plot_c)))\n",
    "    \n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "\n",
    "    # for loop para plotar todas as variáveis\n",
    "    W_List = {}\n",
    "    for column in df_.columns:\n",
    "        W_List[column] = []\n",
    "\n",
    "        if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "\n",
    "            gb = df_.groupby([column]).size()\n",
    "            gb = np.round((gb/gb.sum())*100,1)\n",
    "\n",
    "            gb_s = df_synth.groupby([column]).size()\n",
    "            gb_s = np.round((gb_s/gb_s.sum())*100,1)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            compare = pd.concat([gb,gb_s], axis = 1).fillna(0)\n",
    "            compare.columns = ['Real','Synthetic']\n",
    "            compare['DIFF'] = np.round(abs(compare['Real'] - compare['Synthetic']),2)\n",
    "\n",
    "            \n",
    "            #sns.swarmplot(data=compare, x=\"Real\", y=\"Synthetic\", ax = ax[r,c])\n",
    "            \n",
    "            fig, ax = plt.subplots(1 , 1, figsize = (5,5))\n",
    "            compare[['Real','Synthetic']].plot(kind = 'barh', ax = ax, alpha = 0.8)\n",
    "            ax.set_title(column+\":\"+ str(compare['DIFF'].abs().sum()))\n",
    "            ax.legend()\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(column+'2.png', )\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df_[column])\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(le.transform(df_[column]).reshape(-1,1))\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(le.transform(df_synth[column].dropna()).reshape(-1,1)))\n",
    "                                               ,np.squeeze(mms.transform(le.transform(df_      [column]         ).reshape(-1,1)))\n",
    "                                              )\n",
    "                         )\n",
    "\n",
    "        else:    \n",
    "            plt.savefig(\"marginal_plot_with_focus_on_marginals_Seaborn.png\",figsize=(4,4), dpi=150)\n",
    "\n",
    "            sns.jointplot(x=df_[column], \n",
    "                          y=df_synth[column],\n",
    "                          edgecolor=\"white\",height = 4, marginal_ticks = False\n",
    "                          ,marginal_kws=dict(bins=20)\n",
    "                         ).set_axis_labels(column+' - real', column+' - synthetic', fontsize=16)\n",
    "            #plt.savefig(column+'.png')\n",
    "            \n",
    "            #bins = np.histogram_bin_edges(df_[column], 20)\n",
    "            #sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5, label = 'Real')\n",
    "            #sns.histplot(data=df_synth, x=column, bins = bins, ax = ax[r,c], color = 'red', alpha = 0.4, label = 'Synthetic')\n",
    "            #ax.set_title(column)\n",
    "            #ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "            if c > plot_c-1:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(df_[[column]])\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(df_synth[[column]]))\n",
    "                                               ,np.squeeze(mms.transform(df_[[column]]))\n",
    "                                              )\n",
    "                         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_plots3(title = \"Adults FULL\",\n",
    "df_synth = synthetic_full,\n",
    "df_ = X_test_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_plots2(title = \"Adults FULL\",\n",
    "df_synth = synthetic_full,\n",
    "df_ = X_test_t2, c_size = 3, r_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07.C - Utilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "new_df2 = X_test_t2.copy()\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "new_df2[X_test_t2.select_dtypes(include = ['object']).columns] = enc.fit_transform(X_test_t2.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "base_olga = pd.DataFrame()\n",
    "\n",
    "base = ['Adults Full', 'Adults Outliers 1.5', 'Adults Outliers 3.0']\n",
    "i = 0\n",
    "\n",
    "for data_temp in [synthetic_full, synthetic_out15, synthetic_out30]:\n",
    "    RX_m = new_df2.drop(\"salary\", axis = 1).copy()\n",
    "    RY_m = new_df2[['salary']].copy()\n",
    "\n",
    "    X_trainR, X_testR, Y_trainR, Y_testR  = train_test_split(RX_m, RY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_R = RandomForestClassifier()\n",
    "    RFC_R.fit(X_trainR,Y_trainR)\n",
    "\n",
    "    \n",
    "    synth = data_temp.dropna().copy()\n",
    "    synth[synth.select_dtypes(include = ['object']).columns] = enc.transform(synth.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "    \n",
    "    SX_m = synth.drop(\"salary\", axis = 1).copy()\n",
    "    SY_m = synth[['salary']].copy()\n",
    "\n",
    "\n",
    "    # X_trainS = SX_m.loc[X_trainR.index,:]\n",
    "    # X_testS  = SX_m.loc[X_testR.index,:]\n",
    "    # Y_trainS = SY_m.loc[Y_trainR.index,:]\n",
    "    # Y_testS  = SY_m.loc[Y_testR.index,:]\n",
    "\n",
    "    X_trainS, X_testS, Y_trainS, Y_testS  = train_test_split(SX_m, SY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_S = RandomForestClassifier()\n",
    "    RFC_S.fit(X_trainS,Y_trainS)\n",
    "\n",
    "\n",
    "    #Treino Real, Teste Real\n",
    "    final_predict_RR = pd.DataFrame({'Y':Y_testR.salary.values\n",
    "                                     ,\"RR_pred\":RFC_R.predict(X_testR)\n",
    "                                     ,\"RR_prob\":RFC_R.predict_proba(X_testR)[:,1]}, index = X_testR.index)\n",
    "\n",
    "\n",
    "    #Treino Sintético, Teste Sintético\n",
    "    final_predict_SS = pd.DataFrame({'Y':Y_testS.salary.values,\n",
    "                                     \"SS_pred\":RFC_S.predict(X_testS),\n",
    "                                     \"SS_prob\":RFC_S.predict_proba(X_testS)[:,1]}, index = X_testS.index)\n",
    "\n",
    "\n",
    "    #Treino Sintético, Teste Real\n",
    "    final_predict_SR = pd.DataFrame({'Y':Y_testR.salary.values,\n",
    "                                     \"SR_pred\":RFC_S.predict(X_testR),\n",
    "                                     \"SR_prob\":RFC_S.predict_proba(X_testR)[:,1]}, index = X_testR.index)\n",
    "\n",
    "    final_predict_SR['SR_pred'] = final_predict_SR['SR_pred'].astype(int)\n",
    "    final_predict_SS[['Y','SS_pred']] = final_predict_SS[['Y','SS_pred']].astype(int)\n",
    "\n",
    "    \n",
    "    testR = pd.concat( [X_testR, Y_testR, final_predict_RR, final_predict_SR[['SR_pred','SR_prob']]], axis = 1).copy()\n",
    "    testR['model'] = 'test'\n",
    "    testS = pd.concat( [X_testS, Y_testS,final_predict_SS], axis = 1).copy()\n",
    "    testS['model'] = 'test'\n",
    "    trainR = pd.concat( [X_trainR, Y_trainR], axis = 1).copy()\n",
    "    trainR['model'] = 'train'\n",
    "    trainS = pd.concat( [X_trainS, Y_trainS], axis = 1).copy()\n",
    "    trainS['model'] = 'train'\n",
    "\n",
    "    R = pd.concat([trainR,testR], axis = 0)\n",
    "    R['type'] = 'Real'\n",
    "    R['base'] = base[i]\n",
    "    R['sint_model'] = 'VAE2'\n",
    "\n",
    "\n",
    "    S = pd.concat([trainS,testS], axis = 0)\n",
    "    S['type'] = 'Synt'\n",
    "    S['base'] = base[i]\n",
    "    S['sint_model'] = 'VAE2'\n",
    "    i+=1\n",
    "    \n",
    "    base_olga = base_olga.append(R)\n",
    "    base_olga = base_olga.append(S)\n",
    "    \n",
    "    \n",
    "    df_m = pd.DataFrame([])\n",
    "    metricas = [accuracy_score, f1_score, precision_score, recall_score, roc_auc_score]\n",
    "    for m in metricas:\n",
    "        if m.__name__ != \"roc_auc_score\":\n",
    "            temp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"RR\":m(final_predict_RR.Y, final_predict_RR.RR_pred),\n",
    "                        \"SS\":m(final_predict_SS.Y, final_predict_SS.SS_pred),\n",
    "                        \"SR\":m(final_predict_SR.Y, final_predict_SR.SR_pred)\n",
    "                    }, index = [m.__name__])\n",
    "            df_m = df_m.append(temp)\n",
    "        else:\n",
    "            temp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"RR\":m(final_predict_RR.Y, final_predict_RR.RR_prob),\n",
    "                        \"SS\":m(final_predict_SS.Y, final_predict_SS.SS_prob),\n",
    "                        \"SR\":m(final_predict_SR.Y, final_predict_SR.SR_prob)\n",
    "                    }, index = [m.__name__])\n",
    "            df_m = df_m.append(temp)\n",
    "\n",
    "    print(df_m)\n",
    "\n",
    "# VAE 5HL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07.D - Privacidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "new_df2 = X_test_t2.copy()\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "new_df2[X_test_t2.select_dtypes(include = ['object']).columns] = enc.fit_transform(X_test_t2.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "new_df3 = new_df2.copy()\n",
    "enc2 = MinMaxScaler()\n",
    "new_df3 = pd.DataFrame(enc2.fit_transform(new_df2.copy()), columns = new_df2.columns)\n",
    "\n",
    "base = ['Adults Full', 'Adults Outliers 1.5', 'Adults Outliers 3.0']\n",
    "i = 0\n",
    "\n",
    "base_dist = pd.DataFrame()\n",
    "\n",
    "for data_temp in [synthetic_full, synthetic_out15, synthetic_out30]:\n",
    "    synth = data_temp.dropna().copy()\n",
    "    synth[synth.select_dtypes(include = ['object']).columns] = enc.transform(synth.select_dtypes(include = ['object']).copy())\n",
    "    synth3 = pd.DataFrame(enc2.transform(synth.copy()), columns = synth.columns, index = synth.index)\n",
    "    \n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=2, radius=0.6)\n",
    "    neigh.fit(new_df3)\n",
    "    distances=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[0][0], range(synth3.shape[0]))\n",
    "    ids=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[1][0], range(synth3.shape[0]))\n",
    "    distances_l = pd.DataFrame(list(distances), index = synth3.index)\n",
    "    ids_l = pd.DataFrame(list(ids), index = synth3.index)\n",
    "    \n",
    "    temp = pd.concat([distances_l,ids_l], axis = 1)\n",
    "    temp['base'] = base[i]\n",
    "    i+=1\n",
    "    \n",
    "    base_dist = base_dist.append(temp)\n",
    "    \n",
    "    print(pd.DataFrame(distances_l[0].append(distances_l[1]).quantile(q = [0,0.05,0.25,0.5,0.75,1])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist.columns = ['distancia_1','distancia_2','id_proximo_1','id_proximo_2','base']\n",
    "base_dist.reset_index().to_csv('base_distancia_AE.csv', index = False)\n",
    "base_olga.reset_index().to_csv('base_score_modelo_AE.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
