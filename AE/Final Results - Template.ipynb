{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj1 = \"tratamento A\"\n",
    "proj2 = \"tratamento B\"\n",
    "proj3 = \"tratamento C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.A - Installing Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.B - Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import ks_2samp,wasserstein_distance\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly import tools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.C - Custom Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Classes and Functions\n",
    "from classes import xplor, LowerCase,MissingValue, Outliers, CharEncoder, ScalingTreatmeant, Autoencoder, compare_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params):\n",
    "   \n",
    "    l = [params[s] for s in params.keys() if s.startswith('unit')]\n",
    "\n",
    "    return Autoencoder(\n",
    "        layers_= l +  [params['center']],\n",
    "        activations=[params['activation']]*len(l) + [params['center_activation']],\n",
    "        final_act=params['final_activation'],\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=\"adam\",\n",
    "        learning_rate = params['learning_rate']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_plots(df_, df_synth, title):\n",
    "\n",
    "    # Inputs para gerar o grid de gr치ficos\n",
    "    n_cols = X_test_t2.shape[1]\n",
    "\n",
    "    plot_r = int(np.ceil((n_cols/4)))\n",
    "    plot_c = 4\n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "\n",
    "    # for loop para plotar todas as vari치veis\n",
    "    W_List = {}\n",
    "    fig, ax = plt.subplots(plot_r , plot_c, figsize = (4*plot_c,3*plot_r))\n",
    "    for column in df_.columns:\n",
    "        W_List[column] = []\n",
    "\n",
    "        if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "\n",
    "            gb = df_.groupby([column]).size()\n",
    "            gb = np.round((gb/gb.sum())*100,1)\n",
    "\n",
    "            gb_s = df_synth.groupby([column]).size()\n",
    "            gb_s = np.round((gb_s/gb_s.sum())*100,1)\n",
    "\n",
    "            compare = pd.concat([gb,gb_s], axis = 1).fillna(0)\n",
    "            compare.columns = ['Real','Synthetic']\n",
    "            compare['DIFF'] = np.round(abs(compare['Real'] - compare['Synthetic']),2)\n",
    "\n",
    "            compare[['Real','Synthetic']].plot(kind = 'bar', ax = ax[r,c], alpha = 0.8)\n",
    "            ax[r,c].set_title(column+\":\"+ str(compare['DIFF'].abs().sum()))\n",
    "            ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df_[column])\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(le.transform(df_[column]).reshape(-1,1))\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(le.transform(df_synth[column].dropna()).reshape(-1,1)))\n",
    "                                               ,np.squeeze(mms.transform(le.transform(df_      [column]         ).reshape(-1,1)))\n",
    "                                              )\n",
    "                         )\n",
    "\n",
    "        else:           \n",
    "            bins = np.histogram_bin_edges(df_[column], 20)\n",
    "            sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5, label = 'Real')\n",
    "            sns.histplot(data=df_synth, x=column, bins = bins, ax = ax[r,c], color = 'red', alpha = 0.4, label = 'Synthetic')\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(df_[[column]])\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(df_synth[[column]]))\n",
    "                                               ,np.squeeze(mms.transform(df_[[column]]))\n",
    "                                              )\n",
    "                         )\n",
    "    plt.suptitle(title, fontsize = 15)\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_plots2(df_, df_synth, title, c_size = 4, r_size = 4):\n",
    "\n",
    "    # Inputs para gerar o grid de gr치ficos\n",
    "    n_cols = X_test_t2.shape[1]\n",
    "\n",
    "    plot_r = int(np.ceil((n_cols/4)))\n",
    "    plot_c = 4\n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "\n",
    "    # for loop para plotar todas as vari치veis\n",
    "    W_List = {}\n",
    "    fig, ax = plt.subplots(plot_r , plot_c, figsize = (c_size*plot_c,r_size*plot_r))\n",
    "    for column in df_.columns:\n",
    "        W_List[column] = []\n",
    "\n",
    "        if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "\n",
    "            gb = df_.groupby([column]).size()\n",
    "            gb = np.round((gb/gb.sum())*100,1)\n",
    "\n",
    "            gb_s = df_synth.groupby([column]).size()\n",
    "            gb_s = np.round((gb_s/gb_s.sum())*100,1)\n",
    "\n",
    "            compare = pd.concat([gb,gb_s], axis = 1).fillna(0)\n",
    "            compare.columns = ['Real','Synthetic']\n",
    "            compare['DIFF'] = np.round(abs(compare['Real'] - compare['Synthetic']),2)\n",
    "\n",
    "            compare[['Real','Synthetic']].plot(kind = 'bar', color=['blue','red'], ax = ax[r,c], alpha = 0.8, legend = False)\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].set_xticks([])\n",
    "            ax[r,c].set_yticks([])\n",
    "            ax[r,c].set_xlabel(\"\")\n",
    "            ax[r,c].set_ylabel(\"\")\n",
    "            c+=1\n",
    "\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df_[column])\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(le.transform(df_[column]).reshape(-1,1))\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(le.transform(df_synth[column].dropna()).reshape(-1,1)))\n",
    "                                               ,np.squeeze(mms.transform(le.transform(df_      [column]         ).reshape(-1,1)))\n",
    "                                              )\n",
    "                         )\n",
    "\n",
    "        else:           \n",
    "            bins = np.histogram_bin_edges(df_[column], 20)\n",
    "            sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5)\n",
    "            sns.histplot(data=df_synth, x=column, bins = bins, ax = ax[r,c], color = 'red', alpha = 0.4)\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].set_xlabel(\"\")            \n",
    "            ax[r,c].set_xticks([])\n",
    "            ax[r,c].set_yticks([])\n",
    "            ax[r,c].set_ylabel(\"\")\n",
    "            c+=1\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(df_[[column]])\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(df_synth[[column]]))\n",
    "                                               ,np.squeeze(mms.transform(df_[[column]]))\n",
    "                                              )\n",
    "                         )\n",
    "    plt.suptitle(title, fontsize = 15)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_models(real, synth):\n",
    "    \n",
    "    ce = CharEncoder()\n",
    "    \n",
    "    RX_m = real.drop([ 'salary'], axis = 1).copy()\n",
    "    RX_m = ce.fit_transform(RX_m.copy())\n",
    "    RY_m = real[['salary']].copy()\n",
    "\n",
    "    X_trainR, X_testR, Y_trainR, Y_testR  = train_test_split(RX_m, RY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_R = RandomForestClassifier()\n",
    "    RFC_R.fit(X_trainR,Y_trainR)\n",
    "\n",
    "    SX_m = synth.drop([ 'salary'], axis = 1).copy()\n",
    "    SX_m = ce.transform(SX_m.copy())\n",
    "    SX_m.index = RX_m.index\n",
    "\n",
    "    SY_m = synth[['salary']].copy()\n",
    "    SY_m.index = RY_m.index\n",
    "\n",
    "    X_trainS = SX_m.loc[X_trainR.index,:]\n",
    "    X_testS  = SX_m.loc[X_testR.index,:]\n",
    "    Y_trainS = SY_m.loc[Y_trainR.index,:]\n",
    "    Y_testS  = SY_m.loc[Y_testR.index,:]\n",
    "\n",
    "    RFC_S = RandomForestClassifier(max_depth = 8)\n",
    "    RFC_S.fit(X_trainS,Y_trainS)\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    \n",
    "    final_predict = pd.DataFrame({'REAL':RFC_S.predict(X_testR), \"SYNT\":RFC_S.predict(X_testS)}, index = X_testR.index)\n",
    "    final_predict['CHECK'] = final_predict[\"REAL\"] == final_predict[\"SYNT\"]\n",
    "    \n",
    "    real_predict = pd.DataFrame({'PRED':RFC_S.predict(X_testR), \"REAL\":Y_testR['salary']}, index = X_testR.index)\n",
    "    real_predict['CHECK'] = real_predict[\"REAL\"] == real_predict[\"PRED\"]\n",
    "    \n",
    "    real_predict2 = pd.DataFrame({'PRED':RFC_S.predict_proba(X_testR)[:, 1], \"REAL\":Y_testR['salary']}, index = X_testR.index)\n",
    "    \n",
    "    return [final_predict['CHECK'].sum()/final_predict.shape[0] , real_predict['CHECK'].sum()/real_predict.shape[0], roc_auc_score(real_predict2.REAL, real_predict2.PRED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_models_train(real, synth):\n",
    "    \n",
    "    ce = CharEncoder()\n",
    "    \n",
    "    RX_m = real.drop([ 'salary'], axis = 1).copy()\n",
    "    RX_m = ce.fit_transform(RX_m.copy())\n",
    "    RY_m = real[['salary']].copy()\n",
    "\n",
    "    X_trainR, X_testR, Y_trainR, Y_testR  = train_test_split(RX_m, RY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_R = RandomForestClassifier()\n",
    "    RFC_R.fit(X_trainR,Y_trainR)\n",
    "\n",
    "    SX_m = synth.drop([ 'salary'], axis = 1).copy()\n",
    "    SX_m = ce.transform(SX_m.copy())\n",
    "    SX_m.index = RX_m.index\n",
    "\n",
    "    SY_m = synth[['salary']].copy()\n",
    "    SY_m.index = RY_m.index\n",
    "\n",
    "    X_trainS = SX_m.loc[X_trainR.index,:]\n",
    "    X_testS  = SX_m.loc[X_testR.index,:]\n",
    "    Y_trainS = SY_m.loc[Y_trainR.index,:]\n",
    "    Y_testS  = SY_m.loc[Y_testR.index,:]\n",
    "\n",
    "    RFC_S = RandomForestClassifier(max_depth = 8)\n",
    "    RFC_S.fit(X_trainS,Y_trainS)\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    \n",
    "    final_predict = pd.DataFrame({'REAL':RFC_S.predict(X_trainR), \"SYNT\":RFC_S.predict(X_trainS)}, index = X_trainR.index)\n",
    "    final_predict['CHECK'] = final_predict[\"REAL\"] == final_predict[\"SYNT\"]\n",
    "    \n",
    "    real_predict = pd.DataFrame({'PRED':RFC_S.predict(X_trainR), \"REAL\":Y_trainR['salary']}, index = X_trainR.index)\n",
    "    real_predict['CHECK'] = real_predict[\"REAL\"] == real_predict[\"PRED\"]\n",
    "    \n",
    "    real_predict2 = pd.DataFrame({'PRED':RFC_S.predict_proba(X_trainR)[:, 1], \"REAL\":Y_trainR['salary']}, index = X_trainR.index)\n",
    "    \n",
    "    return [final_predict['CHECK'].sum()/final_predict.shape[0] , real_predict['CHECK'].sum()/real_predict.shape[0], roc_auc_score(real_predict2.REAL, real_predict2.PRED)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#od.download(\"https://www.kaggle.com/rameshmehta/credit-risk-analysis/version/1\")\n",
    "data = pd.read_csv(\"adult.csv\", header = None)\n",
    "data.columns = ['age','workclass','fnlwgt','education','education_num','marital_status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['fnlwgt']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando classe para fazer limepza dos dados de forma automatizada\n",
    "# Essa classe consegue analisar e identificar vari치veis com baixa qualidade em rela칞칚o a valores nulos,\n",
    "# vari치veis categ칩ricas com muitas vari치veis, vari치veis do tipo data, vari치veis com alt칤ssima vari칙ncia \n",
    "# e vari치veis com vari칙ncia nula\n",
    "xp = xplor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando vari치veis nulas.\n",
    "# 칄 realizado um gr치fico de pareto para a % de nulos em cada vari치vel.\n",
    "\n",
    "# De acordo com o par칙metro 'level', ser칚o selecionadas (para exclus칚o) as variaveis que \n",
    "# ultrapassarem o valor desse par칙metro. Ou seja, nesse acso todas as vari치veis com mais de 50%\n",
    "# de nulos ser칚o selecionadas para exclus칚o\n",
    "\n",
    "# Foram encontradas 21 vari치veis.\n",
    "\n",
    "xp.check_nulls(level = 0, select = True)\n",
    "print(xp.nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este pr칩ximo m칠todo visa identificar vari치veis categ칩ricas que possuem uma quantidade alta de valores 칰nicos,\n",
    "# Neste exemplo, toda vari치vel categ칩rica com mais de 20 categrias distintas ser치 selecionada para exclus칚o.\n",
    "\n",
    "# Neste caso foram encontradas 10 vari치veis.\n",
    "\n",
    "# O Gr치fico de pareto 칠 mostrado para ajudar na identifica칞칚o visual\n",
    "\n",
    "xp.check_unique_objects(level_unique = 50,select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O pr칩ximo m칠todo visa identificar vari치veis do tipo data/\n",
    "# Para este experimento, essas vari치veis ser칚o exclu칤das.\n",
    "\n",
    "# Neste exemplo, foram encontradas 5 vari치veis\n",
    "\n",
    "xp.check_dates(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste 칰ltimo m칠todo, o objetivo 칠 encontrar vari치veis que possuam uma vari칙ncia normalizada muito alta ou nula.\n",
    "# O interessante foi verificar que ela se mostrou 칰lti para encontrar \n",
    "# as duas colunas relacionadas ao ID (com alt칤ssima vari칙ncia) e uma coluna sem vari칙ncia.\n",
    "\n",
    "xp.check_var(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para finalizar esse processo, 칠 executado o m칠todo 'clean_data',\n",
    "# que vai pegar todas as vari치veis identificadas nos m칠todos anteriores e vai exclu칤-las da base final\n",
    "\n",
    "new_df = xp.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com isso, foram exclu칤das 34 vari치veis.\n",
    "# A base final agora poss칰i 39 vari치veis.\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.groupby(['education_num','education']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import *\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "df = data.copy()\n",
    "plot_number = 10\n",
    "numerics = df.select_dtypes(exclude = ['object','category']).columns\n",
    "objects  = df.select_dtypes(include = ['object','category']).columns\n",
    "    \n",
    "mms = MinMaxScaler()\n",
    "\n",
    "num_desc = pd.DataFrame(mms.fit_transform(df[numerics]), columns = numerics).describe()\n",
    "\n",
    "obj_desc = df[objects].describe()\n",
    "\n",
    "f0 = figure(num = 0, figsize = (12, 8))#, dpi = 300)\n",
    "#f0.suptitle(\"Oscillation decay\", fontsize=14)\n",
    "ax01 = subplot2grid((2, 2), (0, 0))\n",
    "ax02 = subplot2grid((2, 2), (0, 1))\n",
    "ax03 = subplot2grid((2, 2), (1, 0), colspan=2, rowspan=1)\n",
    "\n",
    "num_desc.loc['std',:].sort_values(ascending = False)[:plot_number].plot(kind = 'bar', ax = ax01)\n",
    "ax01.set_ylabel(\"Vari칙ncia Normalizada\", fontdict = {'size':15})\n",
    "ax01.set_xlabel(\"Vari치veis\", fontdict = {'size':15})\n",
    "ax01.set_title('Vari치veis com maior vari칙ncia normalizada', fontdict = {'size':15})\n",
    "ax01.tick_params(axis='x', labelsize=12, labelrotation = 45)\n",
    "\n",
    "num_desc.loc['std',:].sort_values(ascending = True)[:plot_number].plot(kind = 'bar', ax = ax02)\n",
    "ax02.set_ylabel(\"Vari칙ncia Normalizada\", fontdict = {'size':15})\n",
    "ax02.set_xlabel(\"Vari치veis\", fontdict = {'size':15})\n",
    "ax02.set_title('Vari치veis com menor vari칙ncia normalizada', fontdict = {'size':15})\n",
    "ax02.tick_params(axis='x', labelsize=12, labelrotation = 45)\n",
    "\n",
    "obj_desc.loc['unique',:].sort_values(ascending = False)[:20].plot(kind = 'bar', ax = ax03, label = 'Quantidades')\n",
    "ax03.set_ylabel(\"# Categorias\", fontdict = {'size':15})\n",
    "ax03.set_xlabel(\"Vari치veis\", fontdict = {'size':15})\n",
    "ax03.set_title('Vari치veis com mais categorias', fontdict = {'size':15})\n",
    "ax03.tick_params(axis='x', labelsize=12, labelrotation = 45)\n",
    "f0.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Data Treatmeant Pipeline (Pre-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop(['education_num'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "pipe_full = joblib.load('pipeline_'+proj1+'.sav')\n",
    "pipe_out15 = joblib.load('pipeline_'+proj2+'.sav')\n",
    "pipe_out30 = joblib.load('pipeline_'+proj3+'.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Train Test Split + Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cria칞칚o da amostra treino e teste.\n",
    "# Nesse caso existe apenas as bases 'X', n칚o sendo necess치rio a base 'y', pois o input 칠 o pr칩prio target.\n",
    "# O objetivo do processo 칠 replicar exatamente os dados de entrada.\n",
    "\n",
    "X_train, X_test = train_test_split(new_df.copy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Aqui estamos aplicando o .transform() doa pipeline treinada.\n",
    "# Aqui os dados sofrer칚o as transforma칞칫es necess치rias para ficarem prontos para o treinamento\n",
    "# da rede neural.\n",
    "\n",
    "# Estou mudando um pequeno par칙metro da pipeline para ter a exclus칚o dos outliers para a amostra treino\n",
    "#my_pipe.set_params(Outliers__train=True)\n",
    "pipe_out15.set_params(Outliers__train=True)\n",
    "pipe_out30.set_params(Outliers__train=True)\n",
    "X_train_full = pipe_full.transform(X_train.copy())\n",
    "X_train_out15 = pipe_out15.transform(X_train.copy())\n",
    "X_train_out30 = pipe_out30.transform(X_train.copy())\n",
    "\n",
    "# Estou mudando um pequeno par칙metro da pipeline para n칚o ter a exclus칚o dos outliers na amostra teste\n",
    "#my_pipe.set_params(Outliers__train=False)\n",
    "pipe_out15.set_params(Outliers__train=False)\n",
    "pipe_out30.set_params(Outliers__train=False)\n",
    "X_test_full = pipe_full.transform(X_test.copy())\n",
    "X_test_out15 = pipe_out15.transform(X_test.copy())\n",
    "X_test_out30 = pipe_out30.transform(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_full \",X_train_full.shape)\n",
    "print(\"X_train_out15\",X_train_out15.shape)\n",
    "print(\"X_train_out30\",X_train_out30.shape)\n",
    "\n",
    "\n",
    "print(\"X_test_full \",X_test_full.shape)\n",
    "print(\"X_test_out15\",X_test_out15.shape)\n",
    "print(\"X_test_out30\",X_test_out30.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dtypes = {i:str(X_train.dtypes[i]) for i in X_train.dtypes.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 - AutoEncoder (AE)\n",
    "# Carregando e treinando o melhor AE\n",
    "print(\"Training Model FULL - AutoEncoder (AE)\")\n",
    "with open( proj1 + \"_sparse_1hl_params_model.pkl\", \"rb\") as f:\n",
    "    params_full =  pickle.load(f)\n",
    "ae_full = build_model(params_full)\n",
    "ae_full.fit(X_train_full, epochs = 700)\n",
    "synthetic_full = pipe_full.inverse_transform(ae_full.transform(X_test_full.copy())[0])\n",
    "synthetic_full = synthetic_full.astype(all_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 - AutoEncoder (AE)\n",
    "# Carregando e treinando o melhor AE\n",
    "print(\"Training Model FULL out 1.5 - AutoEncoder (AE)\")\n",
    "with open( proj2 + \"_sparse_1hl_params_model.pkl\", \"rb\") as f:\n",
    "    params_out15 =  pickle.load(f)\n",
    "ae_out15 = build_model(params_out15)\n",
    "ae_out15.fit(X_train_out15, epochs = 700)\n",
    "synthetic_out15 = pipe_out15.inverse_transform(ae_out15.transform(X_test_out15.copy())[0])\n",
    "synthetic_out15 = synthetic_out15.astype(all_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_out15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 - AutoEncoder (AE)\n",
    "# Carregando e treinando o melhor AE\n",
    "print(\"Training Model FULL out 3.0 - AutoEncoder (AE)\")\n",
    "with open( proj3 + \"_sparse_1hl_params_model.pkl\", \"rb\") as f:\n",
    "    params_out30 =  pickle.load(f)\n",
    "ae_out30 = build_model(params_out30)\n",
    "ae_out30.fit(X_train_out30, epochs = 700)\n",
    "synthetic_out30 = pipe_out30.inverse_transform(ae_out30.transform(X_test_out30.copy())[0])\n",
    "synthetic_out30 = synthetic_out30.astype(all_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_out30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07.B - Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t2 = pipe_full.inverse_transform(X_test_full.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculando a dist칙ncia de Wasserstein para todas as colunas para os tr칡s modelos\n",
    "W_df_tst = compare_metric(df_real = X_test_t2\n",
    "                      ,dfs_synth = [synthetic_full, synthetic_out15, synthetic_out30]\n",
    "                      , metric = 'wasserstein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_0: AutoEncoder\n",
    "# Data_1: Sparse AutoEncoder\n",
    "# Data_2: Convolutional AutoEncoder\n",
    "W_df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_df_tst.quantile(q = [0,0.5,1]).append( pd.DataFrame(W_df_tst.mean(), columns = ['avg']).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_plots3(df_, df_synth, title):\n",
    "\n",
    "    # Inputs para gerar o grid de gr치ficos\n",
    "    n_cols = X_test_t2.shape[1]\n",
    "\n",
    "    plot_c = 2\n",
    "    plot_r = int(np.ceil((n_cols/plot_c)))\n",
    "    \n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "\n",
    "    # for loop para plotar todas as vari치veis\n",
    "    W_List = {}\n",
    "    for column in df_.columns:\n",
    "        W_List[column] = []\n",
    "\n",
    "        if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "\n",
    "            gb = df_.groupby([column]).size()\n",
    "            gb = np.round((gb/gb.sum())*100,1)\n",
    "\n",
    "            gb_s = df_synth.groupby([column]).size()\n",
    "            gb_s = np.round((gb_s/gb_s.sum())*100,1)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            compare = pd.concat([gb,gb_s], axis = 1).fillna(0)\n",
    "            compare.columns = ['Real','Synthetic']\n",
    "            compare['DIFF'] = np.round(abs(compare['Real'] - compare['Synthetic']),2)\n",
    "\n",
    "            \n",
    "            #sns.swarmplot(data=compare, x=\"Real\", y=\"Synthetic\", ax = ax[r,c])\n",
    "            \n",
    "            fig, ax = plt.subplots(1 , 1, figsize = (5,5))\n",
    "            compare[['Real','Synthetic']].plot(kind = 'barh', ax = ax, alpha = 0.8)\n",
    "            ax.set_title(column+\":\"+ str(compare['DIFF'].abs().sum()))\n",
    "            ax.legend()\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(column+'2.png', )\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df_[column])\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(le.transform(df_[column]).reshape(-1,1))\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(le.transform(df_synth[column].dropna()).reshape(-1,1)))\n",
    "                                               ,np.squeeze(mms.transform(le.transform(df_      [column]         ).reshape(-1,1)))\n",
    "                                              )\n",
    "                         )\n",
    "\n",
    "        else:    \n",
    "            plt.savefig(\"marginal_plot_with_focus_on_marginals_Seaborn.png\",figsize=(4,4), dpi=150)\n",
    "\n",
    "            sns.jointplot(x=df_[column], \n",
    "                          y=df_synth[column],\n",
    "                          edgecolor=\"white\",height = 4, marginal_ticks = False\n",
    "                          ,marginal_kws=dict(bins=20)\n",
    "                         ).set_axis_labels(column+' - real', column+' - synthetic', fontsize=16)\n",
    "            #plt.savefig(column+'.png')\n",
    "            \n",
    "            #bins = np.histogram_bin_edges(df_[column], 20)\n",
    "            #sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5, label = 'Real')\n",
    "            #sns.histplot(data=df_synth, x=column, bins = bins, ax = ax[r,c], color = 'red', alpha = 0.4, label = 'Synthetic')\n",
    "            #ax.set_title(column)\n",
    "            #ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "            if c > plot_c-1:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "\n",
    "\n",
    "            mms = MinMaxScaler()\n",
    "            mms.fit(df_[[column]])\n",
    "\n",
    "            W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(df_synth[[column]]))\n",
    "                                               ,np.squeeze(mms.transform(df_[[column]]))\n",
    "                                              )\n",
    "                         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_plots3(title = \"Adults FULL\",\n",
    "df_synth = synthetic_full,\n",
    "df_ = X_test_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_plots2(title = \"Adults FULL\",\n",
    "df_synth = synthetic_full,\n",
    "df_ = X_test_t2, c_size = 3, r_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07.C - Utilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "new_df2 = X_test_t2.copy()\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "new_df2[X_test_t2.select_dtypes(include = ['object']).columns] = enc.fit_transform(X_test_t2.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "base_olga = pd.DataFrame()\n",
    "\n",
    "base = ['Adults Full', 'Adults Outliers 1.5', 'Adults Outliers 3.0']\n",
    "i = 0\n",
    "\n",
    "for data_temp in [synthetic_full, synthetic_out15, synthetic_out30]:\n",
    "    RX_m = new_df2.drop(\"salary\", axis = 1).copy()\n",
    "    RY_m = new_df2[['salary']].copy()\n",
    "\n",
    "    X_trainR, X_testR, Y_trainR, Y_testR  = train_test_split(RX_m, RY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_R = RandomForestClassifier()\n",
    "    RFC_R.fit(X_trainR,Y_trainR)\n",
    "\n",
    "    \n",
    "    synth = data_temp.dropna().copy()\n",
    "    synth[synth.select_dtypes(include = ['object']).columns] = enc.transform(synth.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "    \n",
    "    SX_m = synth.drop(\"salary\", axis = 1).copy()\n",
    "    SY_m = synth[['salary']].copy()\n",
    "\n",
    "\n",
    "    # X_trainS = SX_m.loc[X_trainR.index,:]\n",
    "    # X_testS  = SX_m.loc[X_testR.index,:]\n",
    "    # Y_trainS = SY_m.loc[Y_trainR.index,:]\n",
    "    # Y_testS  = SY_m.loc[Y_testR.index,:]\n",
    "\n",
    "    X_trainS, X_testS, Y_trainS, Y_testS  = train_test_split(SX_m, SY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "    RFC_S = RandomForestClassifier()\n",
    "    RFC_S.fit(X_trainS,Y_trainS)\n",
    "\n",
    "\n",
    "    #Treino Real, Teste Real\n",
    "    final_predict_RR = pd.DataFrame({'Y':Y_testR.salary.values\n",
    "                                     ,\"RR_pred\":RFC_R.predict(X_testR)\n",
    "                                     ,\"RR_prob\":RFC_R.predict_proba(X_testR)[:,1]}, index = X_testR.index)\n",
    "\n",
    "\n",
    "    #Treino Sint칠tico, Teste Sint칠tico\n",
    "    final_predict_SS = pd.DataFrame({'Y':Y_testS.salary.values,\n",
    "                                     \"SS_pred\":RFC_S.predict(X_testS),\n",
    "                                     \"SS_prob\":RFC_S.predict_proba(X_testS)[:,1]}, index = X_testS.index)\n",
    "\n",
    "\n",
    "    #Treino Sint칠tico, Teste Real\n",
    "    final_predict_SR = pd.DataFrame({'Y':Y_testR.salary.values,\n",
    "                                     \"SR_pred\":RFC_S.predict(X_testR),\n",
    "                                     \"SR_prob\":RFC_S.predict_proba(X_testR)[:,1]}, index = X_testR.index)\n",
    "\n",
    "    final_predict_SR['SR_pred'] = final_predict_SR['SR_pred'].astype(int)\n",
    "    final_predict_SS[['Y','SS_pred']] = final_predict_SS[['Y','SS_pred']].astype(int)\n",
    "\n",
    "    \n",
    "    testR = pd.concat( [X_testR, Y_testR, final_predict_RR, final_predict_SR[['SR_pred','SR_prob']]], axis = 1).copy()\n",
    "    testR['model'] = 'test'\n",
    "    testS = pd.concat( [X_testS, Y_testS,final_predict_SS], axis = 1).copy()\n",
    "    testS['model'] = 'test'\n",
    "    trainR = pd.concat( [X_trainR, Y_trainR], axis = 1).copy()\n",
    "    trainR['model'] = 'train'\n",
    "    trainS = pd.concat( [X_trainS, Y_trainS], axis = 1).copy()\n",
    "    trainS['model'] = 'train'\n",
    "\n",
    "    R = pd.concat([trainR,testR], axis = 0)\n",
    "    R['type'] = 'Real'\n",
    "    R['base'] = base[i]\n",
    "    R['sint_model'] = 'VAE2'\n",
    "\n",
    "\n",
    "    S = pd.concat([trainS,testS], axis = 0)\n",
    "    S['type'] = 'Synt'\n",
    "    S['base'] = base[i]\n",
    "    S['sint_model'] = 'VAE2'\n",
    "    i+=1\n",
    "    \n",
    "    base_olga = base_olga.append(R)\n",
    "    base_olga = base_olga.append(S)\n",
    "    \n",
    "    \n",
    "    df_m = pd.DataFrame([])\n",
    "    metricas = [accuracy_score, f1_score, precision_score, recall_score, roc_auc_score]\n",
    "    for m in metricas:\n",
    "        if m.__name__ != \"roc_auc_score\":\n",
    "            temp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"RR\":m(final_predict_RR.Y, final_predict_RR.RR_pred),\n",
    "                        \"SS\":m(final_predict_SS.Y, final_predict_SS.SS_pred),\n",
    "                        \"SR\":m(final_predict_SR.Y, final_predict_SR.SR_pred)\n",
    "                    }, index = [m.__name__])\n",
    "            df_m = df_m.append(temp)\n",
    "        else:\n",
    "            temp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"RR\":m(final_predict_RR.Y, final_predict_RR.RR_prob),\n",
    "                        \"SS\":m(final_predict_SS.Y, final_predict_SS.SS_prob),\n",
    "                        \"SR\":m(final_predict_SR.Y, final_predict_SR.SR_prob)\n",
    "                    }, index = [m.__name__])\n",
    "            df_m = df_m.append(temp)\n",
    "\n",
    "    print(df_m)\n",
    "\n",
    "# VAE 5HL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07.D - Privacidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "new_df2 = X_test_t2.copy()\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "new_df2[X_test_t2.select_dtypes(include = ['object']).columns] = enc.fit_transform(X_test_t2.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "new_df3 = new_df2.copy()\n",
    "enc2 = MinMaxScaler()\n",
    "new_df3 = pd.DataFrame(enc2.fit_transform(new_df2.copy()), columns = new_df2.columns)\n",
    "\n",
    "base = ['Adults Full', 'Adults Outliers 1.5', 'Adults Outliers 3.0']\n",
    "i = 0\n",
    "\n",
    "base_dist = pd.DataFrame()\n",
    "\n",
    "for data_temp in [synthetic_full, synthetic_out15, synthetic_out30]:\n",
    "    synth = data_temp.dropna().copy()\n",
    "    synth[synth.select_dtypes(include = ['object']).columns] = enc.transform(synth.select_dtypes(include = ['object']).copy())\n",
    "    synth3 = pd.DataFrame(enc2.transform(synth.copy()), columns = synth.columns, index = synth.index)\n",
    "    \n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=2, radius=0.6)\n",
    "    neigh.fit(new_df3)\n",
    "    distances=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[0][0], range(synth3.shape[0]))\n",
    "    ids=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[1][0], range(synth3.shape[0]))\n",
    "    distances_l = pd.DataFrame(list(distances), index = synth3.index)\n",
    "    ids_l = pd.DataFrame(list(ids), index = synth3.index)\n",
    "    \n",
    "    temp = pd.concat([distances_l,ids_l], axis = 1)\n",
    "    temp['base'] = base[i]\n",
    "    i+=1\n",
    "    \n",
    "    base_dist = base_dist.append(temp)\n",
    "    \n",
    "    print(pd.DataFrame(distances_l[0].append(distances_l[1]).quantile(q = [0,0.05,0.25,0.5,0.75,1])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist.columns = ['distancia_1','distancia_2','id_proximo_1','id_proximo_2','base']\n",
    "base_dist.reset_index().to_csv('base_distancia_AE.csv', index = False)\n",
    "base_olga.reset_index().to_csv('base_score_modelo_AE.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
