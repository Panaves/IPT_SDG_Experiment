{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nome do experimento\n",
    "proj = 'adults_FULL'\n",
    "\n",
    "# delta do IQR para detecção de outliers\n",
    "delta = 0\n",
    "\n",
    "\n",
    "# tipo de espaço latente. \n",
    "# Se 'dense', tenderá a ser menor que tamanho dos dados originais. \n",
    "# Se \"sparse\", pode ser maior que o tamanho dos dados originais\n",
    "center_type = 'dense'\n",
    "\n",
    "# Número de interações do grid-search\n",
    "iterations = 200\n",
    "\n",
    "# Quantidade máxima de hidden layers no Encoder e Decoder\n",
    "hidden = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.A - Installing Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.B - Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import ks_2samp,wasserstein_distance\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly import tools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.C - Custom Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Classes and Functions\n",
    "from classes import xplor, LowerCase,MissingValue, Outliers, CharEncoder, ScalingTreatmeant, VariationalAutoencoder, compare_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params):\n",
    "   \n",
    "    l = [params[s] for s in params.keys() if s.startswith('unit')]\n",
    "\n",
    "    return VariationalAutoencoder(\n",
    "        layers_= l +  [params['center']],\n",
    "        activations=[params['activation']]*len(l) + [params['center_activation']],\n",
    "        final_act=params['final_activation'],\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=\"adam\",\n",
    "        learning_rate = params['learning_rate']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#od.download(\"https://www.kaggle.com/rameshmehta/credit-risk-analysis/version/1\")\n",
    "data = pd.read_csv(\"adult.csv\", header = None)\n",
    "data.columns = ['age','workclass','fnlwgt','education','education_num','marital_status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando classe para fazer limepza dos dados de forma automatizada\n",
    "# Essa classe consegue analisar e identificar variáveis com baixa qualidade em relação a valores nulos,\n",
    "# variáveis categóricas com muitas variáveis, variáveis do tipo data, variáveis com altíssima variância \n",
    "# e variáveis com variância nula\n",
    "xp = xplor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando variáveis nulas.\n",
    "# É realizado um gráfico de pareto para a % de nulos em cada variável.\n",
    "\n",
    "# De acordo com o parâmetro 'level', serão selecionadas (para exclusão) as variaveis que \n",
    "# ultrapassarem o valor desse parâmetro. Ou seja, nesse acso todas as variáveis com mais de 50%\n",
    "# de nulos serão selecionadas para exclusão\n",
    "\n",
    "# Foram encontradas 21 variáveis.\n",
    "\n",
    "xp.check_nulls(level = 0, select = True)\n",
    "print(xp.nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este próximo método visa identificar variáveis categóricas que possuem uma quantidade alta de valores únicos,\n",
    "# Neste exemplo, toda variável categórica com mais de 20 categrias distintas será selecionada para exclusão.\n",
    "\n",
    "# Neste caso foram encontradas 10 variáveis.\n",
    "\n",
    "# O Gráfico de pareto é mostrado para ajudar na identificação visual\n",
    "\n",
    "xp.check_unique_objects(level_unique = 50,select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O próximo método visa identificar variáveis do tipo data/\n",
    "# Para este experimento, essas variáveis serão excluídas.\n",
    "\n",
    "# Neste exemplo, foram encontradas 5 variáveis\n",
    "\n",
    "xp.check_dates(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste último método, o objetivo é encontrar variáveis que possuam uma variância normalizada muito alta ou nula.\n",
    "# O interessante foi verificar que ela se mostrou últi para encontrar \n",
    "# as duas colunas relacionadas ao ID (com altíssima variância) e uma coluna sem variância.\n",
    "\n",
    "xp.check_var(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para finalizar esse processo, é executado o método 'clean_data',\n",
    "# que vai pegar todas as variáveis identificadas nos métodos anteriores e vai excluí-las da base final\n",
    "\n",
    "new_df = xp.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com isso, foram excluídas 34 variáveis.\n",
    "# A base final agora possúi 39 variáveis.\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Data Treatmeant Pipeline (Pre-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop(['education_num'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o pré-processamento dos dados, estou utilizando o método Pipeline do scikit-learn\n",
    "# Este método é interessante pois podemos encadear vários pré-processamentos de dados e criar\n",
    "# um único objeto que irá realizar todos os steps encadeados.\n",
    "\n",
    "# Neste caso, foi-se construído um objeto que realizará 5 pré-processamentos:\n",
    "# - Transformação de todos as variáveis categóricas para lower case\n",
    "# - Input de missing values para numéricas e categóricas\n",
    "# - Identificação e exclusão de outliers (apenas amostra treinamento)\n",
    "# - Aplicação do método OneHotEncoder nos dados categóricos\n",
    "# - Ajsute range dos dados para que eles fiquem entre 0 e 1.\n",
    "\n",
    "#df_new = new_data.drop(['cus_cust_id'], axis = 1).copy()\n",
    "\n",
    "if delta == 0:\n",
    "    my_pipe = Pipeline([ ('LowerCase',LowerCase())\n",
    "#                     ,('MissingValue',MissingValue(num_value = 'value', value_num = 0, obj_value = 'NULL'))\n",
    "#                     ,('Outliers',Outliers(level = 1.5))\n",
    "                    ,('CharEncoder',CharEncoder(methods = 'onehot'))\n",
    "                    ,('ScalingTreatmeant',ScalingTreatmeant())\n",
    "                   ])\n",
    "else:\n",
    "    my_pipe = Pipeline([ ('LowerCase',LowerCase())\n",
    "            #            ,('MissingValue',MissingValue(num_value = 'value', value_num = 0, obj_value = 'NULL'))\n",
    "                        ,('Outliers',Outliers(level = delta))\n",
    "                        ,('CharEncoder',CharEncoder(methods = 'onehot'))\n",
    "                        ,('ScalingTreatmeant',ScalingTreatmeant())\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ao aplicar o .fit(), a pipeline irá aprender diversos aspectos dos dados mas não irá alterá-los ainda.\n",
    "my_pipe.fit(new_df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Train Test Split + Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criação da amostra treino e teste.\n",
    "# Nesse caso existe apenas as bases 'X', não sendo necessário a base 'y', pois o input é o próprio target.\n",
    "# O objetivo do processo é replicar exatamente os dados de entrada.\n",
    "\n",
    "X_train, X_test = train_test_split(new_df.copy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Aqui estamos aplicando o .transform() doa pipeline treinada.\n",
    "# Aqui os dados sofrerão as transformações necessárias para ficarem prontos para o treinamento\n",
    "# da rede neural.\n",
    "\n",
    "# Estou mudando um pequeno parâmetro da pipeline para ter a exclusão dos outliers para a amostra treino\n",
    "if delta > 0:\n",
    "    my_pipe.set_params(Outliers__train=True)\n",
    "X_train_t = my_pipe.transform(X_train.copy())\n",
    "\n",
    "# Estou mudando um pequeno parâmetro da pipeline para não ter a exclusão dos outliers na amostra teste\n",
    "if delta > 0:\n",
    "    my_pipe.set_params(Outliers__train=False)\n",
    "X_test_t = my_pipe.transform(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(my_pipe, 'pipeline_'+proj+'.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_t.shape)\n",
    "print(X_test_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para realização do meu primeiro experimento, necessito escolher dois parâmetros:\n",
    "# 1) Como será o centro do meu autoencoder: pode ser 'dense' ou 'sparse'. \n",
    "#    O 'dense' limita o centro a ser no máximo 40% do tamanho original dos dados de input, \n",
    "#    enquanto o 'sparse' retira esse limitante e deixa livre para o modelo escolher o tamanho\n",
    "#    que for melhor para garantir o melhor resultado.\n",
    "\n",
    "# 2) Quantidade de iteraçoes do Tree of Panzer Element, ou seja, quantas combinações de parâmetros serão utilizadas.\n",
    "#    Como teste, vc pode colocar esse valor como 1. Os modelos que eu treinei serão carregados mais à frente\n",
    "\n",
    "# dimensão do input\n",
    "base_dim = X_train_t.shape[1]\n",
    "\n",
    "# Ajuste do tamanho máximo e mínimo de cada layer, para que a arquitetura final tenha formato de ampulheta\n",
    "# conforme figura mostrada acima\n",
    "ls = np.linspace(1, 10, 10 * hidden) / 10\n",
    "\n",
    "lat_ls = ls[0 : int(len(ls) * 0.4)]\n",
    "hid_aux = ls[int(len(ls) * 0.4) :]\n",
    "\n",
    "hid_aux = list(reversed(hid_aux))\n",
    "hid_ls = []\n",
    "for i in range(0, len(hid_aux), int(len(hid_aux) * (1 / hidden))):\n",
    "    hid_ls.append(hid_aux[i : i + int(len(hid_aux) * (1 / hidden))])\n",
    "\n",
    "# Criação do espaço de possibilidades dos hyper-parametros.\n",
    "# Os hiperparametros são:\n",
    "# - Quantidade de Nodes em cada layer\n",
    "# - Funções de ativação na camada final do enconder\n",
    "# - Funções de ativação na camada final do deconder\n",
    "# - Funções de ativação nos hidden layers\n",
    "\n",
    "space = {\n",
    "    \"units\"\n",
    "    + str(i): hp.choice(\n",
    "        \"units\" + str(i), [int(base_dim * (p)) for p in hid_ls[i]]\n",
    "    )\n",
    "    for i in range(len(hid_ls))\n",
    "}\n",
    "if center_type == \"sparse\":\n",
    "    print([int(base_dim * (p)) for p in ls[0:-1:hidden]])\n",
    "    center = {\n",
    "        \"center\": hp.choice(\n",
    "            \"center\", [int(base_dim * (p)) for p in list(ls[0:-1:hidden]) + [1, 1.1,1.2,1.5,2] ]\n",
    "        )\n",
    "    }\n",
    "elif center_type == \"dense\":\n",
    "    print([int(base_dim * (p)) for p in lat_ls])\n",
    "    center = {\n",
    "        \"center\": hp.choice(\n",
    "            \"center\", [int(base_dim * (p)) for p in lat_ls]\n",
    "        )\n",
    "    }\n",
    "\n",
    "others = {\n",
    "    \"optimizer\": hp.choice(\"optimizer\", [\"adam\"]),\n",
    "    \"activation\": hp.choice(\"activation\", [\"relu\", \"tanh\", \"selu\",\"sigmoid\"]),\n",
    "    \"center_activation\": hp.choice(\n",
    "        \"center_activation\", [\"sigmoid\", \"linear\", \"tanh\", \"relu\",\"selu\"]\n",
    "    ),\n",
    "    \"final_activation\": hp.choice(\n",
    "        \"final_activation\", [\"sigmoid\", \"tanh\"]\n",
    "    ),\n",
    "    \"learning_rate\": hp.choice(\n",
    "        \"learning_rate\", [0.1, 0.05,0.01,0.005,0.001,0.0001,0.00001]\n",
    "    ),\n",
    "}\n",
    "space.update(center)\n",
    "space.update(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar a arquitetura do modelo, treiná-lo e retornar o erro da amostra validação\n",
    "# Esse erro será utilizado pelo Tree of Parzen Element para 'chutar' uma próxima combinação\n",
    "def f_nn(params):\n",
    "    # Criação da arquitetura: quantidade de layers, funções de ativação, função de perda e otimizador\n",
    "    ae = VariationalAutoencoder(\n",
    "        layers_=[params[u] for u in unitss] + [params['center']],\n",
    "        activations=[params['activation']] * len(unitss) +  [params['center_activation']],\n",
    "        final_act=params['final_activation'],\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        learning_rate = params['learning_rate']\n",
    "    )\n",
    "\n",
    "    # treino do modelo\n",
    "    ae.fit(\n",
    "\n",
    "        X_train_t,\n",
    "        Y=None,\n",
    "        epochs=500,\n",
    "        batch_size=300,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        save=0,\n",
    "        proj=0,\n",
    "    )\n",
    "    \n",
    "    # Captura do valor da loss da validação da última iteração do modelo\n",
    "    acc = ae.autoencoder.history.history[\"val_loss\"][len(ae.autoencoder.history.history[\"val_loss\"]) - 1]\n",
    "    return {\"loss\": acc, \"status\": STATUS_OK}\n",
    "     \n",
    "# Criação do objeto hyperopt que realizará o grid-search\n",
    "trials = Trials()\n",
    "\n",
    "algo = partial(\n",
    "    tpe.suggest,\n",
    "    n_startup_jobs=int(iterations * 0.3),\n",
    "    gamma=0.25,\n",
    "    n_EI_candidates=24,\n",
    ")\n",
    "\n",
    "best = fmin(f_nn, space, algo=algo, max_evals=iterations, trials=trials)\n",
    "\n",
    "# Ao final, a função fmin retornará o hyper-parametros que obtiveram o melhor resultado\n",
    "best_p = space_eval(space, best)\n",
    "\n",
    "# Salvando o melhor resultado num arquivo teste\n",
    "# O arquivo orginal será carregado mais abaixo\n",
    "with open( proj + \"_sparse_1hl_params_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(space_eval(space, best), f)\n",
    "best_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
