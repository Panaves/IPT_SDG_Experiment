{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = 'Tratamento A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.A - Installing Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.B - Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import ks_2samp,wasserstein_distance\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly import tools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.C - Custom Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Classes and Functions\n",
    "from classes import xplor, compare_metric\n",
    "from ctabgan import CTABGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_plot_1d(df_, title):\n",
    "\n",
    "    # Inputs para gerar o grid de gráficos\n",
    "    n_cols = df_.shape[1]\n",
    "\n",
    "    plot_r = int(np.ceil((n_cols/4)))\n",
    "    plot_c = 4\n",
    "\n",
    "    c = 0\n",
    "    r = 0\n",
    "\n",
    "    # for loop para plotar todas as variáveis\n",
    "    W_List = {}\n",
    "    fig, ax = plt.subplots(plot_r , plot_c, figsize = (4*plot_c,3*plot_r))\n",
    "    for column in df_.columns:\n",
    "        W_List[column] = []\n",
    "\n",
    "        if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "\n",
    "            gb = df_.groupby([column]).size()\n",
    "            gb = np.round((gb/gb.sum())*100,1)\n",
    "\n",
    "            compare = pd.concat([gb], axis = 1).fillna(0)\n",
    "            compare.columns = ['Real']\n",
    "\n",
    "            compare[['Real']].plot(kind = 'bar', ax = ax[r,c], alpha = 0.8)\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "        else:           \n",
    "            bins = np.histogram_bin_edges(df_[column], 20)\n",
    "            sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5)\n",
    "            ax[r,c].set_title(column)\n",
    "            ax[r,c].legend()\n",
    "\n",
    "            c+=1\n",
    "            if c > 3:\n",
    "                c = 0\n",
    "                r+=1\n",
    "\n",
    "\n",
    "    plt.suptitle(title, fontsize = 15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#od.download(\"https://www.kaggle.com/rameshmehta/credit-risk-analysis/version/1\")\n",
    "data = pd.read_csv(\"Real_Datasets/adult-Copy1.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(4, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = data.columns.copy()\n",
    "colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando classe para fazer limepza dos dados de forma automatizada\n",
    "# Essa classe consegue analisar e identificar variáveis com baixa qualidade em relação a valores nulos,\n",
    "# variáveis categóricas com muitas variáveis, variáveis do tipo data, variáveis com altíssima variância \n",
    "# e variáveis com variância nula\n",
    "xp = xplor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando variáveis nulas.\n",
    "# É realizado um gráfico de pareto para a % de nulos em cada variável.\n",
    "\n",
    "# De acordo com o parâmetro 'level', serão selecionadas (para exclusão) as variaveis que \n",
    "# ultrapassarem o valor desse parâmetro. Ou seja, nesse acso todas as variáveis com mais de 50%\n",
    "# de nulos serão selecionadas para exclusão\n",
    "\n",
    "# Foram encontradas 21 variáveis.\n",
    "\n",
    "xp.check_nulls(level = 0, select = True)\n",
    "print(xp.nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este próximo método visa identificar variáveis categóricas que possuem uma quantidade alta de valores únicos,\n",
    "# Neste exemplo, toda variável categórica com mais de 20 categrias distintas será selecionada para exclusão.\n",
    "\n",
    "# Neste caso foram encontradas 10 variáveis.\n",
    "\n",
    "# O Gráfico de pareto é mostrado para ajudar na identificação visual\n",
    "\n",
    "xp.check_unique_objects(level_unique = 50,select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O próximo método visa identificar variáveis do tipo data/\n",
    "# Para este experimento, essas variáveis serão excluídas.\n",
    "\n",
    "# Neste exemplo, foram encontradas 5 variáveis\n",
    "\n",
    "xp.check_dates(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste último método, o objetivo é encontrar variáveis que possuam uma variância normalizada muito alta ou nula.\n",
    "# O interessante foi verificar que ela se mostrou últi para encontrar \n",
    "# as duas colunas relacionadas ao ID (com altíssima variância) e uma coluna sem variância.\n",
    "\n",
    "xp.check_var(select = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para finalizar esse processo, é executado o método 'clean_data',\n",
    "# que vai pegar todas as variáveis identificadas nos métodos anteriores e vai excluí-las da base final\n",
    "\n",
    "new_df = xp.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn==0.24.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com isso, foram excluídas 34 variáveis.\n",
    "# A base final agora possúi 39 variáveis.\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Data Treatmeant Pipeline (Pre-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"Real_Datasets/Adults_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctbgan = CTABGAN(raw_csv_path = \"Real_Datasets/Adults_full.csv\",\n",
    "                 test_ratio = 0.20,\n",
    "                 categorical_columns = [ 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income'], \n",
    "                 log_columns = [],\n",
    "                 mixed_columns= {'capital-loss':[0.0],'capital-gain':[0.0]},\n",
    "                 integer_columns = ['age', 'fnlwgt','capital-gain', 'capital-loss','hours-per-week'],\n",
    "                 problem_type= {\"Classification\": 'income'},\n",
    "                 epochs = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctbgan.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = ctbgan.generate_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a distância de Wasserstein para todas as colunas para os três modelos\n",
    "W_df_tst = compare_metric(df_real = new_df\n",
    "                      ,dfs_synth = [synth]\n",
    "                      , metric = 'wasserstein')\n",
    "W_df_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_df_tst.quantile(q = [0,0.5,1]).append( pd.DataFrame(W_df_tst.mean().values, index = ['avg'], columns = ['Data_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = new_df.copy()\n",
    "cat_vars = ['workclass','education','marital-status','occupation','relationship','race','gender','native-country','income']\n",
    "new_df_2[cat_vars] = new_df_2[cat_vars].astype(int).astype('object')\n",
    "\n",
    "synth_2 = synth.copy()\n",
    "cat_vars = ['workclass','education','marital-status','occupation','relationship','race','gender','native-country','income']\n",
    "synth_2[cat_vars] = synth_2[cat_vars].astype(int).astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synth.to_csv(\"fake_adult_full.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = pd.read_csv(\"fake_adult_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerando gráficos para o melhor modelo \n",
    "# Desse modo é possível comparar a distribuição de cada variável entre os dados sintéticos e reais\n",
    "df_synth = synth\n",
    "df_ = new_df\n",
    "\n",
    "# Inputs para gerar o grid de gráficos\n",
    "n_cols = new_df.shape[1]\n",
    "\n",
    "plot_r = int(np.ceil((n_cols/4)))\n",
    "plot_c = 4\n",
    "\n",
    "c = 0\n",
    "r = 0\n",
    "\n",
    "# for loop para plotar todas as variáveis\n",
    "W_List = {}\n",
    "fig, ax = plt.subplots(plot_r , plot_c, figsize = (5*plot_c,4*plot_r))\n",
    "for column in df_.columns:\n",
    "    W_List[column] = []\n",
    "\n",
    "    if (df_[column].dtype == 'object') | pd.CategoricalDtype.is_dtype(df_[column]):\n",
    "        \n",
    "        gb = df_.groupby([column]).size()\n",
    "        gb = np.round((gb/gb.sum())*100,1)\n",
    "        \n",
    "        gb_s = df_synth.groupby([column]).size()\n",
    "        gb_s = np.round((gb_s/gb_s.sum())*100,1)\n",
    "\n",
    "        compare = pd.concat([gb,gb_s], axis = 1).fillna(0)\n",
    "        compare.columns = ['Real','Synthetic']\n",
    "        compare['DIFF'] = np.round(abs(compare['Real'] - compare['Synthetic']),2)\n",
    "\n",
    "        compare[['Real','Synthetic']].plot(kind = 'bar', ax = ax[r,c], alpha = 0.8)\n",
    "        ax[r,c].set_title(column+\":\"+ str(compare['DIFF'].abs().sum()))\n",
    "        ax[r,c].legend()\n",
    "        \n",
    "        c+=1\n",
    "\n",
    "        if c > 3:\n",
    "            c = 0\n",
    "            r+=1\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df_[column])\n",
    "\n",
    "        mms = MinMaxScaler()\n",
    "        mms.fit(le.transform(df_[column]).reshape(-1,1))\n",
    "\n",
    "        W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(le.transform(df_synth[column].dropna()).reshape(-1,1)))\n",
    "                                           ,np.squeeze(mms.transform(le.transform(df_      [column]         ).reshape(-1,1)))\n",
    "                                          )\n",
    "                     )\n",
    "\n",
    "    else:           \n",
    "        bins = np.histogram_bin_edges(df_[column], 20)\n",
    "        sns.histplot(data=df_  , x=column, bins = bins, ax = ax[r,c], color = 'blue', alpha = 0.5, label = 'Real')\n",
    "        sns.histplot(data=df_synth, x=column, bins = bins, ax = ax[r,c], color = 'red', alpha = 0.4, label = 'Synthetic')\n",
    "        ax[r,c].set_title(column)\n",
    "        ax[r,c].legend()\n",
    "        \n",
    "        c+=1\n",
    "        if c > 3:\n",
    "            c = 0\n",
    "            r+=1\n",
    "                \n",
    "        \n",
    "\n",
    "        mms = MinMaxScaler()\n",
    "        mms.fit(df_[[column]])\n",
    "\n",
    "        W_List[column].append(wasserstein_distance( np.squeeze(mms.transform(df_synth[[column]]))\n",
    "                                           ,np.squeeze(mms.transform(df_[[column]]))\n",
    "                                          )\n",
    "                     )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "base = ['Adults Full']\n",
    "\n",
    "new_df2 = new_df.copy()\n",
    "\n",
    "new_df2.columns = synth.columns.values\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "new_df2[new_df2.select_dtypes(include = ['object']).columns] = enc.fit_transform(new_df2.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "\n",
    "RX_m = new_df2.drop(\"income\", axis = 1).copy()\n",
    "RY_m = new_df2[['income']].copy()\n",
    "\n",
    "X_trainR, X_testR, Y_trainR, Y_testR  = train_test_split(RX_m, RY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "RFC_R = RandomForestClassifier()\n",
    "RFC_R.fit(X_trainR,Y_trainR)\n",
    "\n",
    "synth2 = synth.dropna().copy()\n",
    "synth2[synth2.select_dtypes(include = ['object']).columns] = enc.transform(synth2.select_dtypes(include = ['object']).copy())\n",
    "\n",
    "\n",
    "SX_m = synth2.drop(\"income\", axis = 1).copy()\n",
    "\n",
    "\n",
    "SY_m = synth2[['income']].copy()\n",
    "\n",
    "\n",
    "# X_trainS = SX_m.loc[X_trainR.index,:]\n",
    "# X_testS  = SX_m.loc[X_testR.index,:]\n",
    "# Y_trainS = SY_m.loc[Y_trainR.index,:]\n",
    "# Y_testS  = SY_m.loc[Y_testR.index,:]\n",
    "\n",
    "X_trainS, X_testS, Y_trainS, Y_testS  = train_test_split(SX_m, SY_m , test_size=0.2, random_state=42)\n",
    "\n",
    "RFC_S = RandomForestClassifier()\n",
    "RFC_S.fit(X_trainS,Y_trainS)\n",
    "\n",
    "\n",
    "#Treino Real, Teste Real\n",
    "final_predict_RR = pd.DataFrame({'Y':Y_testR.income.values\n",
    "                                 ,\"RR_pred\":RFC_R.predict(X_testR)\n",
    "                                 ,\"RR_prob\":RFC_R.predict_proba(X_testR)[:,1]}, index = X_testR.index)\n",
    "\n",
    "\n",
    "#Treino Sintético, Teste Sintético\n",
    "final_predict_SS = pd.DataFrame({'Y':Y_testS.income.values,\n",
    "                                 \"SS_pred\":RFC_S.predict(X_testS),\n",
    "                                 \"SS_prob\":RFC_S.predict_proba(X_testS)[:,1]}, index = X_testS.index)\n",
    "\n",
    "\n",
    "#Treino Sintético, Teste Real\n",
    "final_predict_SR = pd.DataFrame({'Y':Y_testR.income.values,\n",
    "                                 \"SR_pred\":RFC_S.predict(X_testR),\n",
    "                                 \"SR_prob\":RFC_S.predict_proba(X_testR)[:,1]}, index = X_testR.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predict_SR['SR_pred'] = final_predict_SR['SR_pred'].astype(int)\n",
    "final_predict_SS[['Y','SS_pred']] = final_predict_SS[['Y','SS_pred']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testR = pd.concat( [X_testR, Y_testR, final_predict_RR, final_predict_SR[['SR_pred','SR_prob']]], axis = 1).copy()\n",
    "testR['model'] = 'test'\n",
    "testS = pd.concat( [X_testS, Y_testS,final_predict_SS], axis = 1).copy()\n",
    "testS['model'] = 'test'\n",
    "trainR = pd.concat( [X_trainR, Y_trainR], axis = 1).copy()\n",
    "trainR['model'] = 'train'\n",
    "trainS = pd.concat( [X_trainS, Y_trainS], axis = 1).copy()\n",
    "trainS['model'] = 'train'\n",
    "\n",
    "R = pd.concat([trainR,testR], axis = 0)\n",
    "R['type'] = 'Real'\n",
    "R['base'] = base[0]\n",
    "R['sint_model'] = 'VAE2'\n",
    "\n",
    "\n",
    "S = pd.concat([trainS,testS], axis = 0)\n",
    "S['type'] = 'Synt'\n",
    "S['base'] = base[0]\n",
    "S['sint_model'] = 'VAE2'\n",
    "\n",
    "\n",
    "base_olga = pd.DataFrame()\n",
    "base_olga = base_olga.append(R)\n",
    "base_olga = base_olga.append(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.DataFrame([])\n",
    "metricas = [accuracy_score, f1_score, precision_score, recall_score, roc_auc_score]\n",
    "for m in metricas:\n",
    "    if m.__name__ != \"roc_auc_score\":\n",
    "        temp = pd.DataFrame(\n",
    "                {\n",
    "                    \"RR\":m(final_predict_RR.Y, final_predict_RR.RR_pred),\n",
    "                    \"SS\":m(final_predict_SS.Y, final_predict_SS.SS_pred),\n",
    "                    \"SR\":m(final_predict_SR.Y, final_predict_SR.SR_pred)\n",
    "                }, index = [m.__name__])\n",
    "        df_m = df_m.append(temp)\n",
    "    else:\n",
    "        temp = pd.DataFrame(\n",
    "                {\n",
    "                    \"RR\":m(final_predict_RR.Y, final_predict_RR.RR_prob),\n",
    "                    \"SS\":m(final_predict_SS.Y, final_predict_SS.SS_prob),\n",
    "                    \"SR\":m(final_predict_SR.Y, final_predict_SR.SR_prob)\n",
    "                }, index = [m.__name__])\n",
    "        df_m = df_m.append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "new_df3 = new_df2.copy()\n",
    "\n",
    "enc2 = MinMaxScaler()\n",
    "new_df3 = pd.DataFrame(enc2.fit_transform(new_df2.copy()), columns = new_df2.columns)\n",
    "synth3 = pd.DataFrame(enc2.transform(synth2.copy()), columns = synth2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=2, radius=0.6)\n",
    "neigh.fit(new_df3)\n",
    "distances=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[0][0], range(synth3.shape[0]))\n",
    "ids=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[1][0], range(synth3.shape[0]))\n",
    "distances_l = pd.DataFrame(list(distances), index = synth3.index)\n",
    "ids_l = pd.DataFrame(list(ids), index = synth3.index)\n",
    "\n",
    "temp = pd.concat([distances_l,ids_l], axis = 1)\n",
    "temp['base'] = base[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (10,3))\n",
    "distances_l[0].append(distances_l[1]).hist(ax = ax[0])\n",
    "distances_l[0].append(distances_l[1]).plot(kind = 'box', ax = ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.columns = ['distancia_1','distancia_2','id_proximo_1','id_proximo_2','base']\n",
    "temp.reset_index().to_csv('base_distancia_CTABGAN_AF.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(distances_l[0].append(distances_l[1]).quantile(q = [0,0.05,0.25,0.5,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(distances_l[0].append(distances_l[1]).quantile(q = [0,0.05,0.25,0.5,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=2, radius=0.6)\n",
    "neigh.fit(new_df3)\n",
    "distances=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[0][0], range(synth3.shape[0]))\n",
    "ids=map(lambda x: neigh.kneighbors(synth3.iloc[[x]], 2, return_distance=True)[1][0], range(synth3.shape[0]))\n",
    "distances_l = pd.DataFrame(list(distances))\n",
    "ids_l = pd.DataFrame(list(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (10,3))\n",
    "distances_l[0].append(distances_l[1]).hist(ax = ax[0])\n",
    "distances_l[0].append(distances_l[1]).plot(kind = 'box', ax = ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(distances_l[0].append(distances_l[1]).quantile(q = [0,0.05,0.25,0.5,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
